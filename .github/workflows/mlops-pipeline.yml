name: MLOps Pipeline with Model Deployment

on:
  push:
    paths:
      - 'audit_data.csv'  # NUR bei neuen Daten
  workflow_dispatch:

jobs:
  train-and-deploy:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout Repository
      uses: actions/checkout@v3

    - name: Setup Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install Dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run Model Training
      run: |
        echo "Training ML Model..."
        python ml_model_training.py

    - name: Export Model and Update Performance
      run: |
        python -c "
        import joblib
        import json
        import os
        from datetime import datetime
        
        # Modell laden (falls vom Training erstellt)
        if os.path.exists('ml_model.joblib'):
            model = joblib.load('ml_model.joblib')
            
            # Performance aus Training extrahieren (vereinfacht)
            current_accuracy = 0.98  # Hier wÃ¼rden Sie echte Accuracy extrahieren
            
            # Performance History laden
            with open('model_performance.json', 'r') as f:
                perf_data = json.load(f)
            
            # Neue Version erstellen
            new_version = f'v{len(perf_data[\"models\"]) + 1}'
            
            # Modell speichern
            model_filename = f'audit_risk_model_{new_version}.joblib'
            joblib.dump(model, f'models/{model_filename}')
            joblib.dump(model, 'models/audit_risk_model_latest.joblib')
            
            # Performance updaten
            new_model_entry = {
                'version': new_version,
                'accuracy': current_accuracy,
                'timestamp': datetime.now().isoformat(),
                'training_samples': 759,  # Anpassen nach Bedarf
                'model_file': model_filename
            }
            
            perf_data['models'].append(new_model_entry)
            perf_data['latest_version'] = new_version
            
            # Performance History speichern
            with open('model_performance.json', 'w') as f:
                json.dump(perf_data, f, indent=2)
                
            print(f'Model {new_version} exported with accuracy: {current_accuracy}')
        "

    - name: Model Performance Drift Check
      id: drift_check
      run: |
        echo "Checking Model Performance Drift..."
        python -c "
        import json
        
        with open('model_performance.json', 'r') as f:
            perf_data = json.load(f)
        
        models = perf_data['models']
        
        if len(models) >= 2:
            latest_acc = models[-1]['accuracy']
            previous_acc = models[-2]['accuracy']
            performance_drop = previous_acc - latest_acc
            drift_threshold = 0.05  # 5%
            
            drift_detected = performance_drop > drift_threshold
            
            print(f'Previous accuracy: {previous_acc:.3f}')
            print(f'Current accuracy: {latest_acc:.3f}')
            print(f'Performance drop: {performance_drop:.3f}')
            print(f'Drift detected: {drift_detected}')
            
            # Set output for next steps
            import os
            with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                f.write(f'drift_detected={str(drift_detected).lower()}\n')
        else:
            print('Not enough models for drift comparison')
            with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                f.write('drift_detected=false\n')
        "

    - name: Commit New Model
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add models/
        git add model_performance.json
        git commit -m "auto: update model $(date)" || exit 0
        git push

    - name: Model Drift Alert
      if: steps.drift_check.outputs.drift_detected == 'true'
      run: |
        echo "MODEL PERFORMANCE DRIFT DETECTED!"
        echo "Significant performance drop detected."
        echo "Review required before deployment."

    - name: Success Report
      if: steps.drift_check.outputs.drift_detected == 'false'
      run: |
        echo "Model training and deployment successful!"
        echo "New model deployed with stable performance."